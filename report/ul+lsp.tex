\fakesection{Unsupervised Learning and Large Scale Problems}{\hfill\small\texttt{/src/session\_3.m}, \texttt{/src/homework\_3.m}}

\fakesubsection{Kernel principal component analysis}{}

The point of \textit{linear principal component analysis (PCA)} is to find a linear transformation (a projection onto an orthogonal basis) such that the variance of the projected data points is maximised. It's an old method introduced by Pearson. The transformed variables are called principal components. By disregarding some of these it is possible to achieve a dimensionality reduction while still capturing most of the important information carried by the input data. This can be used for denoising purposes.

\begin{figure}[h]
\centering
%
\subfloat[De-noising with linear PCA.]{\includegraphics[width=0.35\textwidth]{../src/figures/kpca/linear}}\\
%
\subfloat[$\sigma^2=0.4,n_h=2$.]{\includegraphics[width=0.25\textwidth]{../src/figures/kpca/kpca_2}}\quad
\subfloat[$\sigma^2=0.4,n_h=6$.]{\includegraphics[width=0.25\textwidth]{../src/figures/kpca/kpca_6}}\quad
\subfloat[$\sigma^2=0.4,n_h=20$.]{\includegraphics[width=0.25\textwidth]{../src/figures/kpca/kpca_20}}\\
%
\subfloat[$\sigma^2=1.0,n_h=2$.]{\includegraphics[width=0.25\textwidth]{../src/figures/kpca/kpca_bis_2}}\quad
\subfloat[$\sigma^2=1.0,n_h=6$.]{\includegraphics[width=0.25\textwidth]{../src/figures/kpca/kpca_bis_6}}\quad
\subfloat[$\sigma^2=1.0,n_h=20$.]{\includegraphics[width=0.25\textwidth]{../src/figures/kpca/kpca_bis_20}}\\
%
\subfloat[$\sigma^2=10.0,n_h=2$.]{\includegraphics[width=0.25\textwidth]{../src/figures/kpca/kpca_tris_2}}\quad
\subfloat[$\sigma^2=10.0,n_h=6$.]{\includegraphics[width=0.25\textwidth]{../src/figures/kpca/kpca_tris_6}}\quad
\subfloat[$\sigma^2=10.0,n_h=20$.]{\includegraphics[width=0.25\textwidth]{../src/figures/kpca/kpca_tris_20}}\\
%
\caption{De-noising a toy dataset consisting of two spirals. The number of data points equals 400, the data point dispersion 0.3. $n_h$ is the number of components. All 9 images below the top one use an RBF kernel. De-noising is done by minimising the distance between the feature mapping of an input and its projection in the feature space onto principal axes. Retaining too few principal components leads to a poor result while retaining too many of them prevents any de-noising.}
\label{kpcatoy}
\end{figure}

\par Like Fisher discriminant analysis (FDA) it is possible to relate this to LS-SVMs and formulate it as a constraint optimisation problem :
$$\max_{w,e}\quad\mathcal{J}_P(w,e)=\gamma\frac{1}{2}\sum_{k=1}^Ne_k^2-\frac{1}{2}w^Tw\qquad\text{such that $e_k=w^T\cdot x_k\quad(k=1,\dots, N)$}$$
The errors represent the variance after projection which is to be maximised. Here too, a dual form can be introduced using Lagrange multipliers. This makes it possible to apply the kernel trick which can make for a non-linear approach (in which case PCA is applied in the feature space). Additionally an increase in the number of dimensions becomes possible when the number of training samples is larger than the number of dimensions. The bias term is responsible for centering the kernel matrix automatically rather than having to do this beforehand. This is especially interesting in the case of kernel spectral clustering (KSC) which is considered in the next paragraph and where centring is not a straightforward thing to do.

\par Because the alternative PCA formulation is model-based it becomes possible to do so-called out-of-sample extensions - the solution is not limited to the input data. It also becomes possible to tune models and find the right parameters that fit the data. However, because of the unsupervised nature of PCA this is not trivial. If the resulting model is meant to be used as a sort of preprocessing step e.g. for use in classification or in the context of a reconstruction problem (assuming an information bottleneck) then tuning can be done on the basis of the related loss function by making use of validation sets (as demonstrated previously). Otherwise it is possible to apply cross-validation techniques based on pre-images to find useful values of the kernel parameters. These pre-images are approximations of inverse transformations of kernel PCA outputs back to the input space and can be used to calculate the reconstruction error for any point in the validation set (which consists of one data point in the case of LOOCV). They can be determined for any input vector $x$ by finding the input vector $z$ which minimises $||\phi(z)-P_{n_c}\cdot \phi(x)||^2$ (where $\phi(x)$ is the feature mapping of input $z$ and $P{n_c}\cdot\phi(x)$ is the projection of $x$ onto a subspace in the feature space spanned by the $n_c$ first eigenvectors found through kPCA.

\par More appropriate (less costly) approaches to tune the parameters include kernel Parallel Analysis (kPA) which is an extension of classical Parallel Analysis (PA). PA is used to find an appropriate number of components for PCA instead of just going by manual inspection of the scree plot or applying a rule like Kaiser's (taking all eigenvectors for which the eigenvalue is larger than 1). Also possible is Model selection based on Distance Distributions (MDD) which improves upon kPA by improving noise estimation when the number of dimensions is low.

\par Consider the de-noising of a toy dataset for which the resulting pre-images after kernel PCA using various parameter values are shown in figure \ref{kpcatoy}. Tuning is done manually. When the number of principal components increases the reconstruction improves which is not surprising. More meaningfully, retaining a smaller number of principal components results in good de-noising performance. As per usual the linear approach (classical PCA) is not able to capture the non-linearities in the input data leading to poor results for this particular dataset. The $\sigma^2$ parameter in the Gaussian kernel can lead to a loss of non-noisy information when set too low (or a lack of de-noising if set too high, as seen in figure \ref{kpcatoy}).

\fakesubsection{Spectral clustering}{}

Classification problems were considered in the first section of this report. In a classification setting a model is constructed which is made to predict what class some data point belongs to. These models are trained in a supervised way i.e. there's an annotated training set and test set which are used to determine appropriate parameters for the model and for evaluating it. In a clustering setting the number of classes $k$ is not necessarily set beforehand but can be tuned. A model is trained in an unsupervised manner such that it clusters data points on the basis of some similarity measure. 

\par Spectral clustering in particular deals with finding a minimal cut of a similarity graph (a weighted \& undirected graph) rather than finding a decision boundary which separates classes. Since finding a balanced yet minimal cut is NP-hard a relaxed version (where the indicators can be real) is considered instead. The solution of the relaxed version is based on the solution of a generalised eigenvalue problem after which the data points can be embedded in an eigenspace on which a clustering algorithm like \textit{k-means} can be applied. In the case of the binary toy dataset dealt with in this section the Fielder vector is used in conjunction with the \texttt{sign} function.

\par Kernel spectral clustering (KSC) reformulates spectral clustering as an LS-SVM, essentially boiling down to a weighted version of kernel PCA. Again, out-of-sample extensions become possible and one can apply it on large-scale problems. The bias term plays the role of a centering operation which is not trivial like in the case of kernel PCA. It is not used in the provided code snippets which causes some issues with thresholding ($k$-means could be used or manual inspection in simple cases).

\clearpage
\begingroup
\setlength{\columnsep}{0.8cm}
\setlength{\intextsep}{0.5cm}
\begin{wrapfigure}{r}{.35\textwidth}
\vspace{-1.0cm}
\begin{minipage}{\linewidth}
    \centering\captionsetup[subfigure]{justification=centering}
    \includegraphics[width=\linewidth]{../src/figures/fixedsize/fixedsize_1}
    \caption*{$\sigma^2=0.01$}
\includegraphics[width=\linewidth]{../src/figures/fixedsize/fixedsize_10}
    \caption*{$\sigma^2=0.1$}
\includegraphics[width=\linewidth]{../src/figures/fixedsize/fixedsize_100}
    \caption*{$\sigma^2=1.0$}
\includegraphics[width=\linewidth]{../src/figures/fixedsize/fixedsize_1000}
    \caption*{$\sigma^2=10.0$}
\includegraphics[width=\linewidth]{../src/figures/fixedsize/fixedsize_10000}
    \caption*{$\sigma^2=100.0$}
\end{minipage}
\caption{Subsample of input data points obtained for a fixed-sized LS-SVM using an RBF kernel of varying $\sigma^2$.}
\label{fixedsample}
\end{wrapfigure}

\par Experiments with an RBF kernel are shown in figure \ref{spectraltoy}. Since the similarity matrix is the Kernel function and $\sigma^2$ happens to control the contrast in similarity between data points a $\sigma^2$ value which is small leads to a single class as all points are considered similar while a larger $\sigma^2$ leads to poor clustering results.

\fakesubsection{Fixed-size LS-SVM}{}

Sometimes the amount of data that is being dealt with is too large such that some of the matrices like the kernel matrix involved in the dual formulations cannot be stored in memory. It becomes desirable, then, to solve the problem in the primal rather than in the dual space (it's the other way round when the number of input dimensions is high). The problem with this is that the feature map $\phi(x)$ is generally not known, only the kernel function representing dot products is made explicit.

\par While decomposition can be used for this an approach that can be considered in the case of fixed-size LS-SVMs is related to the Nystr\"om method which is traditionally used for integral approximation. In the current setting it means that one takes a set of support vectors of the input data (of size $M\ll N$) and then uses it to approximate the feature map $\phi(x)$ such that a parametric solution can be found. For fixed-size LS-SVMs the subsample starts randomly and is then updated iteratively as long as the Renyi entropy improves. This metric can be approximated by taking the sum of the elements of the kernel. This makes it fast to compute which is important in the context of big data (besides accuracy).

\par Examples of subsamples obtained through optimisation of the Renyi entropy are shown in figure \ref{fixedsample}. The kernel is a Gaussian one, the input data is normally distributed. It looks as though a larger value for $\sigma^2$ leads to a selection of data points that are more spread out. This is normal ; maximising the entropy means that one maximises the uncertainty, diversity or un-informativeness of the system. As the bandwidth increases the reach of each data point gets larger, the similarity with distant points increases and maximising entropy - which involves prioritisation of dissimilar points - causes points maximally distantiated from each other to be picked as support vectors.

\par In figure \ref{fscomp} a fixed-size LS-SVM approach is compared with a variant which applies an $\ell_0$-penalty after generating a fixed-size LS-SVM solution. The goal is to increase sparsity as determining $M$ isn't trivial. The post-processing does not influence the total runtime because its complexity is $\mathcal{O}(M^3)$ while that of the fixed-size LS-SVM is $\mathcal{O}(NM^2)$. While the error is about the same the number of support vector is much lower for the latter method. This is not surprising since the $\ell_0$-norm counts the number of non-zero values which means that it aims for sparse representations. It may not be the most sparse representation possible ; as obtaining $\ell_0$ is NP-hard only a local minimum is found through an iterative procedure.

\fakesubsection{Kernel principal component analysis}{}

De-noising a quite noisy USPS dataset with classical PCA gives poor results at best (see figure \ref{usps_linear}). Kernel PCA with an RBF kernel manages to exploit the data much more and the results are a big improvement (see figure \ref{usps_kernel}).

\endgroup

What was noted in the yin-yang toy dataset shown at the beginning of this section can be observed again in figure \ref{uspsband} ; small bandwidths remove important information (such that some digits get reconstructed without noise but as the wrong number) while larger ones fail to de-noise and noise is re-introduced slowly but surely in the reconstructions. Eventually the results resemble that of classical, linear PCA.

\par The small bandwidths cause the data points to be considered highly dissimilar and directions of largest variation do not correspond to clusters with the same digits - rather, every data point is its own little cluster and variation is large in many meaningless directions. Conversely, very high bandwidths cause the data points to be considered very similar to each other and it once again gets hard to find meaningful directions of variation such that noise shared by the data points may be considered informative again.

\begin{figure}[h]
\centering
%
\subfloat[Error estimation.]{\includegraphics[width=0.25\textwidth]{../src/figures/fixedsize/comparison_error}}\quad
\subfloat[Number of support vectors.]{\includegraphics[width=0.25\textwidth]{../src/figures/fixedsize/comparison_sv}}\quad
\subfloat[Time taken for generating the models.]{\includegraphics[width=0.25\textwidth]{../src/figures/fixedsize/comparison_time}}\\
%
\caption{Comparison of the standard fixed-size LS-SVM approach with a modified approach applying an $\ell_0$-penalty to obtain a sparser representation.}
\label{fscomp}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{../src/figures/usps/linear_pca}
        \caption{Results of linear PCA applied to the USPS dataset (top rows are original and noisy data, next 9 rows are results for $n_c\in [1,2,4,8,16,32,64,128,190]$).}
\label{usps_linear}
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{../src/figures/usps/kernel_pca}
        \caption{Results of kernel PCA applied to the USPS dataset (top rows are original and noisy data, next 9 rows are results for $n_c\in [1,2,4,8,16,32,64,128,190]$).}
\label{usps_kernel}
    \end{minipage}
\end{figure}

\par Of note ; for slightly too small bandwidths, say $f=0.01$ or $f=0.1$ it looks like reconstructions are simply training data samples. This appears to be caused by the algorithm that calculates pre-images. It's a fixed-point iteration algorithm that takes a linear combination of the training data at each step.

\begin{figure}[h]
\centering
%
\subfloat[$f=0.01$.]{\includegraphics[width=0.25\textwidth]{../src/figures/usps/kernel_pca_001}}\hfil
\subfloat[$f=0.1$.]{\includegraphics[width=0.25\textwidth]{../src/figures/usps/kernel_pca_01}}\hfil
\subfloat[$f=1.0$.]{\includegraphics[width=0.25\textwidth]{../src/figures/usps/kernel_pca_1}}\hfil
\subfloat[$f=10.0$.]{\includegraphics[width=0.25\textwidth]{../src/figures/usps/kernel_pca_10}}\\
%
%\subfloat[$f=100.0$.]{\includegraphics[width=0.30\textwidth]{../src/figures/usps/kernel_pca_5}}\quad
%\subfloat[$f=1000.0$.]{\includegraphics[width=0.30\textwidth]{../src/figures/usps/kernel_pca_6}}\quad
%\subfloat[$f=10000.0$.]{\includegraphics[width=0.30\textwidth]{../src/figures/usps/kernel_pca_7}}
%
\caption{De-noising the USPS hand-written digit dataset. $f$ is a factor with which is multiplied by about 51 to get the bandwidth. Very low and very high factors $f$ (e.g. 0.001 and 1000) lead to noisy reconstructions (resembling the second rows) and aren't pictured here.}
\label{uspsband}
\end{figure}

\par It was stated previously that one way to tune kPCA is to measure reconstruction errors for the validation set to gauge the effectiveness of certain parameter combinations. A plot of the reconstruction errors measured for the provided training and validation sets is shown in figure \ref{uspserror}. The errors tell a meaningful story which also corresponds to the visualisation of the performance of various parameter combinations in figure \ref{uspsband}.

\par Using the tuned parameters based on the contour plots better results are obtained. This can be seen in figure \ref{uspsresult}. The noise has been removed for the most part though one (and a half) de-noised digit is wrong.

\begin{figure}[h]
\centering
\subfloat[Results with linear PCA ($n_c=16$, first validation set) for comparison's sake.]{\includegraphics[width=0.7\linewidth]{../src/figures/usps/result_linear}}\hfil
\subfloat[Results for validation set 1.]{\includegraphics[width=0.7\linewidth]{../src/figures/usps/result_test1}}\hfil
\subfloat[Results for validation set 2.]{\includegraphics[width=0.7\linewidth]{../src/figures/usps/result_test2}}\hfil
\caption{Denoising of the USPS dataset using tuned parameters ($n_c=70,f=0.4$).}
\label{uspsresult}
\end{figure}

\begin{figure}[!htb]
\centering
\subfloat[Errors (root mean squared errors) for the training set.]{\includegraphics[width=0.32\textwidth]{../src/figures/usps/contour_xtrain}}\hfil
\subfloat[Errors (root mean squared errors) for the first test set.]{\includegraphics[width=0.32\textwidth]{../src/figures/usps/contour_xtest1}}\hfil
\subfloat[Errors (root mean squared errors) for the second test set.]{\includegraphics[width=0.32\textwidth]{../src/figures/usps/contour_xtest2}}\hfil
\caption{Reconstruction error for the training and validation sets in function of the bandwidth and the number of components. This is a crude grid search (x-axis represents the factor $f$, y-axis is the number of components) in combination with interpolation to construct a contour plot. The axes are scaled logarithmically. More appropriate tuning methods include kernel parallel analysis or Model selection based on Distance Distributions (MDD). As the number of components rises, the bandwidth can be higher before de-noising fails.}
\label{uspserror}
\end{figure}

\begin{figure}[h]
\vspace{-0.5cm}
\centering
%
\subfloat[$\sigma^2=0.001$.]{
\begin{minipage}{0.48\textwidth}
\includegraphics[width=0.45\textwidth]{../src/figures/spectral/rings_clusters_1}
\includegraphics[width=0.4\textwidth]{../src/figures/spectral/rings_block_1}
\end{minipage}}\quad
\subfloat[$\sigma^2=0.005$.]{
\begin{minipage}{0.48\textwidth}
\includegraphics[width=0.45\textwidth]{../src/figures/spectral/rings_clusters_5}
\includegraphics[width=0.4\textwidth]{../src/figures/spectral/rings_block_5}
\end{minipage}}\\
%
\subfloat[$\sigma^2=0.01$.]{
\begin{minipage}{0.48\textwidth}
\includegraphics[width=0.45\textwidth]{../src/figures/spectral/rings_clusters_10}
\includegraphics[width=0.4\textwidth]{../src/figures/spectral/rings_block_10}
\end{minipage}}\quad
\subfloat[$\sigma^2=1.0$.]{
\begin{minipage}{0.48\textwidth}
\includegraphics[width=0.45\textwidth]{../src/figures/spectral/rings_clusters_1000}
\includegraphics[width=0.4\textwidth]{../src/figures/spectral/rings_block_1000}
\end{minipage}}\\
%
\caption{Clustering of a toy dataset consisting of two rings. KSC was applied using a kernel with varying $\sigma^2$ values. When this bandwidth is too large the reach of influence of each data point becomes too large and the clustering becomes subpar. The block-diagonals that are shown are similarity (or affinity) matrices after they've been sorted based on cluster membership. The blue rings for $\sigma^2$ are due to poor clustering because of using the \texttt{sign} function to threshold. With a better threshold the clustering works fine. For even smaller bandwidths the clustering deteriorates again. Projections onto the principal subspace are supposed to be collinear in the case of ideal clustering and are not shown here.}
\label{spectraltoy}
\end{figure}

\fakesubsection{Fixed-size LS-SVM}{}

Next, two datasets are experimented with. LS-SVM with or without sparisification is applied to the Shuttle - and then the California housing dataset. The first is a classification problem, the second a regression problem.

\fakesubsubsection{Shuttle dataset}{}

The Shuttle dataset has 58.000 train or test samples. It's a multi-class classification problem, having a total of 7 classes which are not equally represented. The first class makes for 45.586 out of 58.000 samples or about $78.5\%$ of the dataset so generally 80\% is seen as the so-called majority rule and the aim is to reach an accuracy of at least 99\%. The class imbalance can be observed in the distributions (figure \ref{shuttlehist}).

\par The provided code reduces the dataset's size to 700 samples in which only 5 of the classes are present (more than 81\% being the majority class), presumably because of the long runtimes. It does not seem to handle multi-class problems per se but still makes use of the sign function for predictions effectively treating the problem as a sort one-versus-all binary classification problem (though because the class labels aren't set to minus one it's not even exactly that). Therefore initial results see a fairly large error rate which tend to get worse than the majority rule. However, from these results which are shown in figure \ref{shuttleestimates} it can be seen that the $\ell_0$ post-processing step does not increase the required time (nor does windowing), which mirrors the results in the study proposing these methods. The required time across values for $k$ (which specifies the number of prototype vectors indirectly through the relation $M=k\cdot\sqrt{N}$) matches the theoretical complexity $\mathcal{O}(k^2\cdot N^2)$. The post-processing also makes the number of support vectors drop, and the error tends to increase a bit.

\par In further experiments instead of using the provided set, at least 3000 samples were picked (such that class 3 was represented more than once and stratification could be applied). For parameter tuning 5-fold cross-validation was applied (minimising misclassification), whereas the evaluation was done by creating a 75/25\% training \& test set split through stratified sampling.

%\par For training, validation and evaluation a training set of 29.000 - and training and validation sets of both 14.500 samples were used (split randomly by stratification).

\begin{figure}[!htb]
\centering
%
\subfloat[First feature (time).]{\includegraphics[width=1.0\textwidth]{../src/figures/fssvm/shuttle_hist_1}}\\
\subfloat[Third feature (time).]{\includegraphics[width=1.0\textwidth]{../src/figures/fssvm/shuttle_hist_3}}\\
\subfloat[Seventh feature (time).]{\includegraphics[width=1.0\textwidth]{../src/figures/fssvm/shuttle_hist_7}}
\caption{3 out of 9 features of the Shuttle dataset. 48.586, 50, 171, 8.903, 3.267, 10 and 13 samples belong to class 1, 2, 3, 4, 5, 6 and 7 respectively. The class names are Rad Flow, Fpv Close, Fpv Open, High, Bypass, Bpv Close, Bpv Open. The data appears reasonably separable, especially when using non-linear kernels.}
\label{shuttlehist}
\end{figure}

To get more meaningful error estimates and an overall comparison between a fixed-size LS-SVM with or without sparsification a multi-class model was built which starts with distinguishing the majority class, then proceeds to distinguish each of the next classes taking the largest classes first. In other words ; class 1 was compared with the others, a model was built on that basis, then for all samples not belonging to class 1 this same procedure was applied, this time for the second largest class (class 4). And so on until only two classes remain. The whole dataset was used except for classes 6 \& 7 as misclassifying them would barely affect the accuracy anyways. In reality the cost of these misclassifications could be considered more serious which would call for an other approach (the ROC curve can always be used during validation to pick useful operating points). Average errors, number of support vectors and time requirements are shown in figure \ref{shuttlemultiestimates} and a short comparison of accuracy with some other approaches is shown below.

\begin{table}[htb]
\centering
\begin{tabular}{c|ccc}
\textit{Method} & \textit{Accuracy} & \textit{Computational Time} & \textit{Support Vectors} \\\hline
kNN ($k=1$) & 99.86\% & \textit{negligible} & \textit{NA} \\
kNN ($k=2$) & 99.82\% & \textit{negligible} & \textit{NA} \\
kNN ($k=3$) & 99.81\% & \textit{negligible} & \textit{NA} \\
FS LS-SVM multiclass ($k=2$) & 99.77\% & 7.69s & 740.00 \\
FS LS-SVM multiclass ($k=4$) & 99.82\% & 18.03s & 1466.67 \\
FS LS-SVM multiclass ($k=6$)$^{\dagger}$ & 99.83\% & 31.37s & 2176.00 \\
FS LS-SVM multiclass ($\ell_0$ reduced, $k=2$) & 99.61\% & 9.15s & 451.67 \\
FS LS-SVM multiclass ($\ell_0$ reduced, $k=4$) & 99.74\% & 33.09s & 815.67 \\
FS LS-SVM multiclass ($\ell_0$ reduced, $k=6$) & 99.70\% & 87.72s & 1804.00 \\
FS LS-SVM multiclass ($\ell_0$ reduced, $k=8$) & 99.67\% & 158.27s & 2791.67 \\
\end{tabular}
\caption{Comparison of some methods for classification of the Shuttle dataset. Averaged over 3 runs. Tuning the models was done through 5-fold cross-validation and evaluation was done on 25\% of the data (14.494 samples). RBF kernels were used (linear models reached an accuracy of a little above 90\%). Classes 6 \& 7 were disregarded. For kNN the algorithm keeps on degrading as $k$ (the number of neighbours) increases. At $k=4$ it tends to start performing worse than the fixed-sized LS-SVMs. Note that the number of support vectors is for all SVMs together and the time for tuning wasn't measured. $^{\dagger}$Only 1 run was used for this setting due to long runtimes.}
\label{shuttlemulticomparison}
\end{table}

\par A general trend that was seen in all experiments is that increasing $k$ tends to decrease the error rate up until a certain point (usually until $k=6$). The results speak for themselves and are proportional to the one-versus-the-rest approach discussed previously when it comes to computational time and the number of support vectors.

\par Ironically a simple $k$-nearest-neighbor (kNN) search using the \texttt{MATLAB} function \texttt{knnsearch} performed better for low values of $k$ (this time denoting the number of neighbours that are considered). A bit surprising maybe, though the dataset is not particularly hard to classify and kNN can be quite powerful due to its complex decision boundaries. On the flipside it takes in more memory as the training set is quite large in comparison with any of the measured number of support vectors. And it cannot be tuned as much ; only $k$ and the distance function can be.

\begin{figure}[!htb]
\centering
\begin{minipage}{\textwidth}
        \centering
        \subfloat[Error estimation.]{\includegraphics[width=0.32\textwidth]{../src/figures/fssvm/shuttle_comparison_error_x}}\hfil
	\subfloat[Estimation of the number of support vectors.]{\includegraphics[width=0.32\textwidth]{../src/figures/fssvm/shuttle_comparison_sv_x}}\hfil
	\subfloat[Time estimation.]{\includegraphics[width=0.32\textwidth]{../src/figures/fssvm/shuttle_comparison_time_x}}\hfil
\end{minipage}
\caption{Estimates of the error, number of support vectors and computational time for a multiclass SVM approach. The estimates are based on a 75/25\% train \& test split obtained through stratified sampling. The number of prototype vectors is $M=k\cdot\sqrt({N}$ with $k=2$.}
\label{shuttlemultiestimates}
\end{figure}

In the case of fixed-sized LS-SVMs $\gamma$, the type of kernel and its parameters, $k$ and even some other things such as the tuning algorithm can be changed. However, because of costly tuning processes no more experiments were done. 

\fakesubsubsection{California housing dataset}{}

The California housing dataset contains information about block groups in California, spanning a population of about 30 million people in total. The 8 features include such things as median income, mean number of rooms in houses, ... and the dependent variable is the median house value. In other words, a 9-dimensional regression problem. The linear dependences (correlation coefficients) of each feature with the median house value are shown below in table \ref{calicorrcoef}. Median income correlates most strongly. It looks like the `ocean proximity' attribute has been removed as it is a text attribute.

\begingroup
\begin{wraptable}{r}{0.5\textwidth}
%\vspace{0.6cm}
\centering
\begin{tabular}{c|c}
\textit{Feature} & \textit{Correlation}\\\hline
\textit{Longitude} & -0.04597\\
\textit{Latitude} & -0.14416\\
\textit{Housing Median Age} & 0.10562\\
\textit{Total Rooms} & 0.13415\\
\textit{Total Bedrooms} & 0.05059\\
\textit{Population} & -0.02465\\
\textit{Households} & 0.06584\\
\textit{Median Income} & 0.68808\\
\textit{Median House Value} & 1.00000
\end{tabular}
\caption{Correlation coefficients between any feature and the dependent variable (median house value).}
\label{calicorrcoef}
\end{wraptable}

\par At first it looks like there are outliers in the mean house value distribution. These could be removed or a reweighing scheme could be used. It's unclear why they're there. Maybe a cap was set at 500k and anything above that was rounded down. Or maybe they're genuine values due to the the big cities in California (which seems fairly unlikely). The regions where houses are pricey are shown in figure \ref{calipricey} - they coincide with Los Angeles and San Fransisco. Regions with a higher median income that are located close to the ocean.

\par For testing purposes a test set was generated by doing a random split. This turned out to be fairly representative (based on the means of the variables for the train and the test set as well as visual inspection of the resulting histograms). There are more rigorous ways to do stratified sampling for multivariate regression problems which are not dealt with here.

\endgroup

\par Initial experiments obtained by running the provided code resulted in an \texttt{mse} of $0.31$ for RBF kernels and bad results for linear ones. The results are not visualised here as they're not very interpretable and the influence of $k$ on time - and memory complexity was discussed before. Instead of using the mean squared error the mean absolute error was used inversion of the standardisation procedure (in \texttt{initial.m}) was applied to facilitate interpretability. The results showed the average error to be about 42k. Without the median house values of at least 500k this dropped by about 3-4k. For polynomial kernels the average mean absolute error turned out to be comparable. At all times, tuned parameters were used.

\begingroup
\begin{wrapfigure}{r}{0.45\textwidth}
\vspace{-1cm}
\centering
\includegraphics[width=0.9\linewidth]{../src/figures/fssvm/california_hist_1}\hfil
%\includegraphics[width=0.45\textwidth]{../src/figures/fssvm/california_hist_6}
\captionof{figure}{Histogram for the median house value.}
\includegraphics[width=0.9\linewidth]{../src/figures/fssvm/california_pricey}\hfil
\captionof{figure}{Regions with pricey houses.}
\label{calipricey}
\end{wrapfigure}

%\par The average time estimates, number of support vectors and error estimates turned out to be just the same as always ; the $\ell_0$ post-processing tends to reduce the number of support vectors without affecting computational complexity much while the error tends to increase a tad (but not always).

\par There's an other way to tackle large-scaled problems that is a little more straightforward, and that is to split the dependent variable into intervals and create a so-called committee network. After these are tuned and trained they can be combined in a linear or even a non-linear way. The latter is generally applied through the use of a multi-layer perceptron (MLP).

\par This approach was also toyed with using the linear approach outlined in \textit{Least-Squares Support Vector Machines} (Suykens, 2003). Where the following covariance matrix is calculated :
$$C_{ij}=\frac{1}{N}\sum_{k=1}^N[f_i(x_k)-y_k]\cdot[f_j(x_k)-y_k]$$
Then the weights for each model were found through the following formula : 
$$\beta = \frac{C^{-1}\cdot 1_{\nu}}{1_{\nu}^T\cdot C^{-1}\cdot 1_{\nu}}\quad\text{where}\quad 1_{\nu} = [1\ 1\ ... 1]^T$$
The validity of the implementation was tested by re-doing the regression on a noisy \texttt{sinc} problem the results of which are shown in figure \ref{committeesinc}.

\endgroup

\begingroup
\setlength{\columnsep}{0.8cm}
\setlength{\intextsep}{0.5cm}
\begin{wrapfigure}{r}{.45\textwidth}
\vspace{-1.0cm}
\begin{minipage}{\linewidth}
    \centering\captionsetup[subfigure]{justification=centering}
\includegraphics[width=\linewidth]{../src/figures/committee/sinc_2_models}
%    \caption*{$\#\ \text{models} = 2$}
\includegraphics[width=\linewidth]{../src/figures/committee/sinc_4_models}
%    \caption*{$\#\ \text{models} = 4$}
\includegraphics[width=\linewidth]{../src/figures/committee/sinc_8_models}
%    \caption*{$\#\ \text{models} = 8$}
\end{minipage}
\caption{Regression on the \texttt{sinc} function using committee networks with a varying number of models (2, 4 and 8). Prediction in red, \texttt{sinc} function in green, noisy \texttt{sinc} in blue.}
\label{committeesinc}
\end{wrapfigure}

\par The committee network performed compa upon the FS LS-SVM approach when 4 models were used, having a mean absolute error of 38.460 on the test set. However, due to the long runtime (a couple of minutes) no check for statistical significance (or even a simple average of the results) was determined. In practice to compare models it's best to have many runs and do a t-test or something equivalent (possibly a non-parametric test depending on the assumptions that are deemed appropriate). More tests were done on smaller subsets of the training data which showed that the results depended greatly on the parameters. The kernel parameters are always important but in this case also the number of models (which was taken to be a power of 2) greatly affected performance. Toy experiments on the \texttt{sinc} function shown here on the right visualise this. In general splitting the dataset into 4 partitions appeared to perform best. At all times these partitions were generated randomly, not by taking fixed intervals (as per the recommendations).

\par What became apparent is that the standardisation procedure (in \texttt{initial.m}) in the case of the committee network sometimes reduced predictive power. This could be due to features with smaller ranges being less important or due to the normalisation (which brings the data to zero mean and unit variance, component-wise) not being sophisticated enough as it assumes independence of the features.

\par A limited experiment using an MLP with as inputs the outputs of the SVM models was tried as well (using the Deep Learning toolbox in \texttt{MATLAB} which is rather easy to use). This to obtain a non-linear combination. The model was trained with the Adam optimiser using a 75/25\% train \& validation split and the results turned out to be comparable to the linear approach as the mean absolute error turned out to be about 42k. This may not be representative as once again, only one run was done.

\par Finally, ARD was quickly tried but no features were removed. Of course none of the obtained models are anywhere near close to perfect so many improvements could be obtained through further experimentation or even by using entirely different frameworks altogether. 

\endgroup

\begin{figure}[!htb]
\vspace{-0.5cm}
\centering
\begin{minipage}{\textwidth}
        \centering
        \subfloat[Error estimation.]{\includegraphics[width=0.32\textwidth]{../src/figures/fssvm/shuttle_comparison_error_2}}\hfil
	\subfloat[Estimation of the number of support vectors.]{\includegraphics[width=0.32\textwidth]{../src/figures/fssvm/shuttle_comparison_sv_2}}\hfil
	\subfloat[Time estimation.]{\includegraphics[width=0.32\textwidth]{../src/figures/fssvm/shuttle_comparison_time_2}}\hfil
	\caption*{$k=2$}
\end{minipage}
\begin{minipage}{\textwidth}
        \centering
        \subfloat[Error estimation.]{\includegraphics[width=0.32\textwidth]{../src/figures/fssvm/shuttle_comparison_error_4}}\hfil
	\subfloat[Estimation of the number of support vectors.]{\includegraphics[width=0.32\textwidth]{../src/figures/fssvm/shuttle_comparison_sv_4}}\hfil
	\subfloat[Time estimation.]{\includegraphics[width=0.32\textwidth]{../src/figures/fssvm/shuttle_comparison_time_4}}\hfil
	\caption*{$k=4$}
\end{minipage}
\begin{minipage}{\textwidth}
        \centering
        \subfloat[Error estimation.]{\includegraphics[width=0.32\textwidth]{../src/figures/fssvm/shuttle_comparison_error_6}}\hfil
	\subfloat[Estimation of the number of support vectors.]{\includegraphics[width=0.32\textwidth]{../src/figures/fssvm/shuttle_comparison_sv_6}}\hfil
	\subfloat[Time estimation.]{\includegraphics[width=0.32\textwidth]{../src/figures/fssvm/shuttle_comparison_time_6}}\hfil
	\caption*{$k=6$}
\end{minipage}
\begin{minipage}{\textwidth}
        \centering
        \subfloat[Error estimation.]{\includegraphics[width=0.32\textwidth]{../src/figures/fssvm/shuttle_comparison_error_8}}\hfil
	\subfloat[Estimation of the number of support vectors.]{\includegraphics[width=0.32\textwidth]{../src/figures/fssvm/shuttle_comparison_sv_8}}\hfil
	\subfloat[Time estimation.]{\includegraphics[width=0.32\textwidth]{../src/figures/fssvm/shuttle_comparison_time_8}}\hfil
	\caption*{$k=8$}
\end{minipage}
\caption{Boxplots for estimates of the error, number of support vectors and time needed to generate FS-SVMs with or without $\ell_0$ norm post-processing procedure. Estimates are based on results on a test set (20\% of 3000 samples, obtained through stratified sampling). Estimates based on 10-fold cross-validation are about equivalent. The dataset is restricted to 3000 samples to avoid long runtimes. These belong to the first 5 classes. At all times an RBF kernel is used which has automatically tuned parameters. The increase in time requirements for increasing value of $k$ corresponds to the theoretical complexity $\mathcal{O}(k^2N^2)$.}
\label{shuttleestimates}
\end{figure}