\fakesection{Function Estimation and Time Series Prediction}{\hfill\small\texttt{/src/session\_2/.m}}

\fakesubsection{Support vector machine for function estimation}{}

Two datasets are experimented with in figure \ref{functionestimation}. The first exists of 20 points lying on a slope. Therefore one can expect the linear kernel (which generates a linear model) to outperform the others. The second (non-linear) dataset is more challenging such that other kernels which enable the SVM to model the non-linearities have to be used instead. 

\par From the problem statement :
$$\min_{w,b,\xi,\xi^*}\frac{1}{2}w\cdot w^T+C\cdot\sum_{k=1}^N\xi+\xi^*$$
where $\xi,\xi^*$ are slack variables representing deviation from the so-called \textit{$\epsilon$-insensitive tube} it can be deduced that the $C$ (the \textit{bound}) plays the usual role of a regularisation parameter, prioritising either the smoothness of the model or how much it fits the data (i.e. minimising the slack variables). $C=0$ leads to simple horizontal lines for the linear kernel which express the view that the two input features $x_1$ and $x_2$ aren't related. The $\epsilon$ parameter controls the width of the tube in which the data points are made to lie. A larger value decreases the number of support vectors and makes the model less accurate.

\par The formulation resembles that of a least squares fit with Thikonov regularisation but the $\epsilon$-insensitive tube makes for a different loss function that encourages sparsity and since the problem is turned into a constraint optimisation problem a dual form can be expressed for which one can apply the kernel trick to be able to model any nonlinearities.

\begin{figure}[h]
\centering
\subfloat[Linear kernel.]{\includegraphics[width=0.16\textwidth]{../src/figures/estimation/gui_linear}}\qquad
\subfloat[Polynomial kernel, $\delta=3$.]{\includegraphics[width=0.16\textwidth]{../src/figures/estimation/gui_polynomial}}\qquad
\subfloat[RBF kernel, $\sigma^2=1$.]{\includegraphics[width=0.16\textwidth]{../src/figures/estimation/gui_rbf}}\qquad
\subfloat[Trigonometric kernel, degree 1.]{\includegraphics[width=0.16\textwidth]{../src/figures/estimation/gui_trigonometric}}\qquad
\subfloat[Exponential kernel, $\sigma^2=1$.]{\includegraphics[width=0.16\textwidth]{../src/figures/estimation/gui_exponential}}\qquad
\\
\subfloat[Linear kernel.]{\includegraphics[width=0.16\textwidth]{../src/figures/estimation/gui_nonlinear_linear}}\qquad
\subfloat[Polynomial kernel, $\delta=3$.]{\includegraphics[width=0.16\textwidth]{../src/figures/estimation/gui_nonlinear_polynomial}}\qquad
\subfloat[RBF kernel, $\sigma^2=0.1$.]{\includegraphics[width=0.16\textwidth]{../src/figures/estimation/gui_nonlinear_rbf}}\qquad
\subfloat[Linear b-spline kernel, degree 5.]{\includegraphics[width=0.16\textwidth]{../src/figures/estimation/gui_nonlinear_bspline}}\qquad
\subfloat[Exponential kernel, $\sigma^2=1.0$.]{\includegraphics[width=0.16\textwidth]{../src/figures/estimation/gui_nonlinear_exponential}}\qquad
\caption{Visualisation of the results of experiments with various kernels and parameters for two different datasets. In the first row $\epsilon=0$ and \texttt{bound} = 10, in the second one $\epsilon=1$ and \texttt{bound} = 1000. Some of the models are likely to be overfitted though these are just toy examples without any test set.}
\label{functionestimation}
\end{figure}

\fakesubsection{A simple example: the sinc function}{}

Figure \ref{sincestimate} shows the results of LS-SVM regression applied on the \texttt{sinc} function. The mean squared error is lowest for a case that is clearly overfitting. In this case the underlying function is known and it seems reasonable to take $\sigma\in\{0.1,1.0\}$ and, say, $\gamma=10^3$. However, an optimal parameter cannot be found as it depends strongly on the training set.

\par Results of automated tuning are shown hereunder :

\begin{table}[h]
\centering
\begin{tabular}{c|ccc}
\textit{Method} & $\gamma$ & $\sigma^2$ & \textit{mean squared error} \\
\hline
Grid search & 5528319.4668 $\pm$ 54371026.5606 & 0.1239 $\pm$  0.1513 & 0.0104 $\pm$ 0.0001\\
Nelder-Mead & 2329.7378 $\pm$ 2318.2696 & 0.1049 $\pm$ 0.0892 &  0.0104 $\pm$ 0.0001\\
\end{tabular}
\caption{Results of automated tuning strategies (averaged over 40 runs).}
\label{automatedtuning}
\end{table}

The results are comparable to those obtained previously in the context of classification, with a few outliers for $\gamma$. Again, grid search appeared to be slower (185 seconds versus 110 for the simplex method). Some of the models seem to overfit the data a bit.

\begin{figure}[h]
\centering
\subfloat[$\gamma=10,\sigma=0.01,\rho=0.0117$]{\includegraphics[width=0.3\textwidth]{../src/figures/estimation/regression_1}}\qquad
\subfloat[$\gamma=10,\sigma=1,\rho=0.0296$]{\includegraphics[width=0.3\textwidth]{../src/figures/estimation/regression_2}}\qquad
\subfloat[$\gamma=10,\sigma=100,\rho=0.1273$]{\includegraphics[width=0.3\textwidth]{../src/figures/estimation/regression_3}}\qquad
\\
\subfloat[$\gamma=10^3,\sigma=0.01,\rho=0.0120$]{\includegraphics[width=0.3\textwidth]{../src/figures/estimation/regression_4}}\qquad
\subfloat[$\gamma=10^3,\sigma=1,\rho=0.0114$]{\includegraphics[width=0.3\textwidth]{../src/figures/estimation/regression_5}}\qquad
\subfloat[$\gamma=10^3,\sigma=100,\rho=0.1107$]{\includegraphics[width=0.3\textwidth]{../src/figures/estimation/regression_6}}\qquad
\\
\subfloat[$\gamma=10^6,\sigma=0.01,\rho=0.0125$]{\includegraphics[width=0.3\textwidth]{../src/figures/estimation/regression_7}}\qquad
\subfloat[$\gamma=10^6,\sigma=1,\rho=0.0099$]{\includegraphics[width=0.3\textwidth]{../src/figures/estimation/regression_8}}\qquad
\subfloat[$\gamma=10^6,\sigma=100,\rho=0.1048$]{\includegraphics[width=0.3\textwidth]{../src/figures/estimation/regression_9}}\qquad
\caption{Function estimation experiments with the \texttt{sinc} function. Noisy samples are fed to an LS-SVM. The green line is the estimated model, the blue dots represent the test data. $\rho$ is the mean squared error. Small $\sigma$ values make the model fit the noise.}
\label{sincestimate}
\end{figure}

When making use of a validation set that very set cannot be used for training. Making use of a Bayesian framework is an alternative and allows one to infer appropriate parameter combinations while making full use of the dataset. Depending on the kernel that is being used either 2 or 3 levels of inference are applied. At each of these level Bayes' theorem is used to infer parameter values. The equations are : 

\begin{equation}
p(w,b|\mathcal{D},\mu,\zeta_{1\_N},\mathcal{H}_{\sigma}) = \frac{p(\mathcal{D}|w,b,\mu,\zeta_{1\_N},\mathcal{H}_{\sigma})}{\textcolor{cyan}{p(\mathcal{D}|\mu,\zeta_{1\_N},\mathcal{H}_{\sigma})}}\cdot p(w,b|\mu,\zeta_{1\_N},\mathcal{H}_{\sigma})
\end{equation}
\begin{equation}
p(\mu,\zeta_{1\_N}|\mathcal{D},\mathcal{H}_{\sigma})=\frac{\textcolor{cyan}{p(\mathcal{D}|\mu,\zeta_{1\_N},\mathcal{H}_{\sigma})}}{\textcolor{teal}{p(\mathcal{D}|\mathcal{H}_{\sigma})}}\cdot p(\mu,\zeta_{1\_N}|\mathcal{H}_{\sigma})
\end{equation}
\begin{equation}
p(\mathcal{H}_{\sigma}|\mathcal{D})=\frac{\textcolor{teal}{p(\mathcal{D}|\mathcal{H}_{\sigma})}}{p(D)}\cdot p(\mathcal{H}_{\sigma})
\end{equation}

As indicated by the colourings the evidence at any level equals the likelihood in the next level. At the first level one can take the logarithm of the product to see the relation with the primal form in the LS-SVM problem specification ; while the prior corresponds to the regularisation term the likelihood corresponds to the least squares cost term.

\begingroup
\setlength{\columnsep}{0.75cm}
\setlength{\intextsep}{0cm}
\begin{wrapfigure}{l}{0pt}
\centering
\includegraphics[width=0.4\textwidth]{../src/figures/estimation/bayesian_inference}
\caption{Results of Bayesian inference applied on training data based on the \texttt{sinc} function with some added white noise (blue). The function estimate is green, error bars are red.}
\label{bayesianinference}
\end{wrapfigure}

One nice facet of this approach is that one ends up with error bars indicating uncertainty. This can be seen in figure \ref{bayesianinference}.

\endgroup

\fakesubsection{Automatic Relevance Determination}{}



\fakesubsection{Robust regression}{}

By applying cross-validation on any subset of the input features one can consider the most promising subset as having the relevant features.

\fakesubsection{Introduction: time series prediction}{}


\fakesubsection{Logmap dataset}{}


\fakesubsection{Santa Fe dataset}{}



